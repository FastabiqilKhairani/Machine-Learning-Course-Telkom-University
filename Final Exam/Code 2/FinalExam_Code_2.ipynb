{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalExam_Code_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRcDRX8Cpf8X",
        "outputId": "1d004bef-7104-41fb-dbeb-020b4aee0417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: dataset/train-processed-processed.csv\n"
          ]
        }
      ],
      "source": [
        "#Proses untuk Training Data\n",
        "!python2 code/preprocess.py dataset/train-processed.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proses untuk Test Data\n",
        "!python2 code/preprocess.py dataset/test-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUaGahiQrIYd",
        "outputId": "9099aa86-4dd7-4814-ea3a-b557b80373a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"code/preprocess.py\", line 107, in <module>\n",
            "    preprocess_csv(csv_file_name, processed_file_name, test_file=False)\n",
            "  File \"code/preprocess.py\", line 81, in preprocess_csv\n",
            "    positive = int(line[:line.find(',')])\n",
            "ValueError: invalid literal for int() with base 10: 'is so sad for my apl friend'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proses untuk training data yang sudah di-train\n",
        "!python2 code/stats.py dataset/train-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfLwLSCKrKux",
        "outputId": "775ca2bd-123c-4f2d-aeb9-a6ccf68a3736"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/train-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/train-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 0, Avg: 0.0000, Max: 0\n",
            "URLs => Total: 0, Avg: 0.0000, Max: 0\n",
            "Emojis => Total: 0, Positive: 0, Negative: 0, Avg: 0.0000, Max: 0\n",
            "Words => Total: 1283269, Unique: 50361, Avg: 12.8327, Max: 41, Min: 0\n",
            "Bigrams => Total: 1183437, Unique: 392543, Avg: 11.8344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proses untuk test data yang sudah di-train\n",
        "!python2 code/stats.py dataset/test-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvlBe0k6sFJu",
        "outputId": "fc649d79-8271-4b30-fb1c-c8a604dab307"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/test-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/test-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 0, Positive: 0, Negative: 0\n",
            "Traceback (most recent call last):\n",
            "  File \"code/stats.py\", line 106, in <module>\n",
            "    print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
            "ZeroDivisionError: float division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Menggunakan Baseline\n",
        "!python2 code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbDeLwVGsSA-",
        "outputId": "8cdd4f9f-3d56-41dc-d6b9-889601360aff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan naive bayes\n",
        "!python2 code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3kqePi-sZFA",
        "outputId": "30ba3e05-686d-4641-da5a-9593d876338b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7772/10000 = 77.7200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan logistic\n",
        "!python3 code/logistic.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFJFTd0Bso0-",
        "outputId": "e5ff4d2a-7694-4dfd-d46e-704c1285b99d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-22 07:33:38.951193: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 51/180, loss:0.6569, acc:0.67002022-01-22 07:33:47.237062: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 83/180, loss:0.6476, acc:0.66402022-01-22 07:33:51.085693: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5913, acc:0.75602022-01-22 07:34:03.278222: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 1, val_acc:0.7342\n",
            "Accuracy improved from 0.0000 to 0.7342, saving model\n",
            "Iteration 41/180, loss:0.5760, acc:0.74602022-01-22 07:34:10.193419: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 83/180, loss:0.5608, acc:0.77202022-01-22 07:34:15.169081: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 87/180, loss:0.5527, acc:0.77202022-01-22 07:34:15.659405: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 117/180, loss:0.5652, acc:0.73202022-01-22 07:34:19.264878: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5544, acc:0.75002022-01-22 07:34:28.746080: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 2, val_acc:0.7504\n",
            "Accuracy improved from 0.7342 to 0.7504, saving model\n",
            "Iteration 72/180, loss:0.5406, acc:0.79202022-01-22 07:34:38.005075: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 105/180, loss:0.5342, acc:0.75002022-01-22 07:34:41.984716: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5116, acc:0.78802022-01-22 07:34:52.462628: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:34:53.034085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 3, val_acc:0.7578\n",
            "Accuracy improved from 0.7504 to 0.7578, saving model\n",
            "Iteration 33/180, loss:0.5225, acc:0.76002022-01-22 07:34:57.495085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 146/180, loss:0.5117, acc:0.75802022-01-22 07:35:11.014201: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4841, acc:0.79802022-01-22 07:35:15.963148: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 4, val_acc:0.7632\n",
            "Accuracy improved from 0.7578 to 0.7632, saving model\n",
            "Iteration 4/180, loss:0.4852, acc:0.81602022-01-22 07:35:17.866221: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 60/180, loss:0.4662, acc:0.81402022-01-22 07:35:24.456693: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 86/180, loss:0.4809, acc:0.80202022-01-22 07:35:27.576053: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.5067, acc:0.76602022-01-22 07:35:40.255227: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:35:40.371349: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 5, val_acc:0.7674\n",
            "Accuracy improved from 0.7632 to 0.7674, saving model\n",
            "Iteration 63/180, loss:0.4799, acc:0.77402022-01-22 07:35:48.412631: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 132/180, loss:0.4790, acc:0.78602022-01-22 07:35:56.534096: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4606, acc:0.7920\n",
            "Epoch: 6, val_acc:0.7689\n",
            "Accuracy improved from 0.7674 to 0.7689, saving model\n",
            "Iteration 165/180, loss:0.4602, acc:0.81002022-01-22 07:36:23.911910: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4520, acc:0.82002022-01-22 07:36:26.251086: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:36:26.361086: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:36:27.233440: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 7, val_acc:0.7720\n",
            "Accuracy improved from 0.7689 to 0.7720, saving model\n",
            "Iteration 6/180, loss:0.4626, acc:0.76802022-01-22 07:36:28.675971: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 37/180, loss:0.4388, acc:0.82202022-01-22 07:36:32.288694: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 99/180, loss:0.4855, acc:0.77402022-01-22 07:36:39.653088: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 152/180, loss:0.4420, acc:0.81402022-01-22 07:36:45.877433: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4212, acc:0.8440\n",
            "Epoch: 8, val_acc:0.7735\n",
            "Accuracy improved from 0.7720 to 0.7735, saving model\n",
            "Iteration 3/180, loss:0.4287, acc:0.82402022-01-22 07:36:51.806495: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 10/180, loss:0.4409, acc:0.82202022-01-22 07:36:52.615938: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 31/180, loss:0.4398, acc:0.80602022-01-22 07:36:55.070075: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 36/180, loss:0.4415, acc:0.83202022-01-22 07:36:55.653279: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 93/180, loss:0.4373, acc:0.81802022-01-22 07:37:02.308499: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 117/180, loss:0.4610, acc:0.79002022-01-22 07:37:05.111523: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 128/180, loss:0.4752, acc:0.79402022-01-22 07:37:06.396885: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 137/180, loss:0.4479, acc:0.79802022-01-22 07:37:07.446081: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 176/180, loss:0.4676, acc:0.78002022-01-22 07:37:12.039536: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4587, acc:0.7900\n",
            "Epoch: 9, val_acc:0.7745\n",
            "Accuracy improved from 0.7735 to 0.7745, saving model\n",
            "Iteration 11/180, loss:0.4666, acc:0.80802022-01-22 07:37:16.129485: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 46/180, loss:0.4351, acc:0.81402022-01-22 07:37:20.220029: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 51/180, loss:0.4514, acc:0.81002022-01-22 07:37:20.812637: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 117/180, loss:0.4582, acc:0.79002022-01-22 07:37:28.529055: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 130/180, loss:0.4353, acc:0.81002022-01-22 07:37:30.067999: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4295, acc:0.81402022-01-22 07:37:35.979717: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:37:37.527096: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 10, val_acc:0.7755\n",
            "Accuracy improved from 0.7745 to 0.7755, saving model\n",
            "Iteration 15/180, loss:0.4139, acc:0.83202022-01-22 07:37:40.006628: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4335, acc:0.8080\n",
            "Epoch: 11, val_acc:0.7757\n",
            "Accuracy improved from 0.7755 to 0.7757, saving model\n",
            "Iteration 21/180, loss:0.4181, acc:0.81402022-01-22 07:38:04.227081: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 66/180, loss:0.4400, acc:0.80602022-01-22 07:38:09.507794: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4482, acc:0.79002022-01-22 07:38:24.848868: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 12, val_acc:0.7770\n",
            "Accuracy improved from 0.7757 to 0.7770, saving model\n",
            "Iteration 20/180, loss:0.4284, acc:0.82402022-01-22 07:38:27.512216: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 30/180, loss:0.4216, acc:0.82802022-01-22 07:38:28.685136: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4353, acc:0.8040\n",
            "Epoch: 13, val_acc:0.7765\n",
            "Iteration 46/180, loss:0.4186, acc:0.81402022-01-22 07:38:54.006254: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 176/180, loss:0.4078, acc:0.81202022-01-22 07:39:09.211818: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 179/180, loss:0.4012, acc:0.84202022-01-22 07:39:09.576062: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4002, acc:0.83202022-01-22 07:39:10.356948: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 14, val_acc:0.7775\n",
            "Accuracy improved from 0.7770 to 0.7775, saving model\n",
            "Iteration 18/180, loss:0.4168, acc:0.82602022-01-22 07:39:14.048614: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 71/180, loss:0.4372, acc:0.81202022-01-22 07:39:20.215740: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 98/180, loss:0.4058, acc:0.82602022-01-22 07:39:23.391461: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4101, acc:0.83402022-01-22 07:39:33.609086: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:39:34.700083: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 15, val_acc:0.7771\n",
            "Iteration 54/180, loss:0.4384, acc:0.80602022-01-22 07:39:41.696443: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 91/180, loss:0.4046, acc:0.82802022-01-22 07:39:46.051090: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3955, acc:0.82802022-01-22 07:39:57.238371: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 16, val_acc:0.7776\n",
            "Accuracy improved from 0.7775 to 0.7776, saving model\n",
            "Iteration 55/180, loss:0.4221, acc:0.80402022-01-22 07:40:05.352067: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 56/180, loss:0.4189, acc:0.82002022-01-22 07:40:05.475452: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 65/180, loss:0.3999, acc:0.82402022-01-22 07:40:06.523086: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 116/180, loss:0.4179, acc:0.82802022-01-22 07:40:12.496727: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3938, acc:0.83002022-01-22 07:40:19.934719: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:40:20.707971: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 17, val_acc:0.7774\n",
            "Iteration 4/180, loss:0.4311, acc:0.80002022-01-22 07:40:22.702614: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 6/180, loss:0.3998, acc:0.83402022-01-22 07:40:22.930173: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 87/180, loss:0.3792, acc:0.85802022-01-22 07:40:32.420353: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3573, acc:0.86802022-01-22 07:40:45.456077: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 18, val_acc:0.7772\n",
            "Iteration 16/180, loss:0.4008, acc:0.84602022-01-22 07:40:47.496083: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 21/180, loss:0.4220, acc:0.81802022-01-22 07:40:48.100768: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 58/180, loss:0.3984, acc:0.85202022-01-22 07:40:52.489969: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 71/180, loss:0.3861, acc:0.82802022-01-22 07:40:54.010897: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 174/180, loss:0.4011, acc:0.82602022-01-22 07:41:06.133095: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 177/180, loss:0.3625, acc:0.86602022-01-22 07:41:06.496564: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.3915, acc:0.84802022-01-22 07:41:07.753085: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 19, val_acc:0.7776\n",
            "Iteration 122/180, loss:0.3670, acc:0.86802022-01-22 07:41:23.626057: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "Iteration 180/180, loss:0.4139, acc:0.83002022-01-22 07:41:31.925079: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 07:41:32.040084: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 20, val_acc:0.7775\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to logistic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan Decision Tree\n",
        "!python2 code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7fM8ZQsuusS",
        "outputId": "50197604-b302-49b5-881b-fb3718847f37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6716/10000 = 67.1600 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan Random Forest\n",
        "!python2 code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3I_hhCpvOhK",
        "outputId": "b30077aa-c0b5-4176-b485-ea21bda8f9bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7200/10000 = 72.0000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan XGBoost\n",
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = 'dataset/train-processed-processed.csv'\n",
        "TEST_PROCESSED_FILE = 'dataset/test-processed-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 1500\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 100\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            utils.write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(20)\n",
        "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = utils.split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=20)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        utils.save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2P93xabvUKQ",
        "outputId": "2681de83-3ab7-4415-9dde-b7fa6a0225d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7126/10000 = 71.2600 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan SVM\n",
        "!python2 code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rtz_v4CxTDm",
        "outputId": "16d965d3-8c49-4020-a35b-207b37ae8034"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7852/10000 = 78.5200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan Neural Network\n",
        "!python3 code/neuralnet.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBQIKRTmzEYP",
        "outputId": "262e3674-51f4-4f4f-cbf9-c82fa9f8be6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-22 08:01:45.760260: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 180/180, loss:0.5369, acc:0.7420\n",
            "Epoch: 1, val_acc:0.7580\n",
            "Accuracy improved from 0.0000 to 0.7580, saving model\n",
            "Iteration 180/180, loss:0.4783, acc:0.76202022-01-22 08:02:18.524654: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-01-22 08:02:18.734729: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 2, val_acc:0.7684\n",
            "Accuracy improved from 0.7580 to 0.7684, saving model\n",
            "Iteration 180/180, loss:0.4726, acc:0.7780\n",
            "Epoch: 3, val_acc:0.7693\n",
            "Accuracy improved from 0.7684 to 0.7693, saving model\n",
            "Iteration 180/180, loss:0.4350, acc:0.7920\n",
            "Epoch: 4, val_acc:0.7695\n",
            "Accuracy improved from 0.7693 to 0.7695, saving model\n",
            "Iteration 180/180, loss:0.4747, acc:0.76802022-01-22 08:03:05.131427: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 5, val_acc:0.7646\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download file Glove untuk model LSTM\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg1OpJFtz7sc",
        "outputId": "d2344bad-ddee-49a8-82e6-16035c3d6bfa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-22 08:08:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-22 08:08:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-22 08:08:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.78MB/s    in 2m 40s  \n",
            "\n",
            "2022-01-22 08:11:22 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Membuka file ZIP\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuxICHsl1xZD",
        "outputId": "00b69759-05c3-43d6-9753-6e91809e3a57"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan model LSTM dengan file glove.6B.200d.txt\n",
        "!python3 code/lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeVSI_aW2FFA",
        "outputId": "03d98d53-8e56-4310-f16e-33fc13d567a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 400000/0\n",
            "\n",
            "Found 31735 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-22 08:18:43.964763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 200)           18000200  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 200)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               168448    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,176,969\n",
            "Trainable params: 18,176,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "704/704 [==============================] - 16s 16ms/step - loss: 0.5397 - accuracy: 0.7266 - val_loss: 0.4810 - val_accuracy: 0.7678\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4582 - accuracy: 0.7857 - val_loss: 0.4594 - val_accuracy: 0.7797\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4085 - accuracy: 0.8156 - val_loss: 0.4716 - val_accuracy: 0.7808\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3635 - accuracy: 0.8410 - val_loss: 0.4779 - val_accuracy: 0.7761\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3230 - accuracy: 0.8603 - val_loss: 0.5005 - val_accuracy: 0.7734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan model LSTM dengan file glove.6B.200d.txt\n",
        "!python3 code/lstm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKMto5Vs3Wts",
        "outputId": "5498cf13-e038-4c7c-df79-9260c9554c41"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 400000/0\n",
            "\n",
            "Found 31735 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-22 08:24:04.207199: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 100)           9000100   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 100)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,125,669\n",
            "Trainable params: 9,125,669\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "704/704 [==============================] - 11s 11ms/step - loss: 0.5599 - accuracy: 0.7084 - val_loss: 0.4866 - val_accuracy: 0.7667\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 8s 11ms/step - loss: 0.4796 - accuracy: 0.7739 - val_loss: 0.4933 - val_accuracy: 0.7709\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 8s 11ms/step - loss: 0.4366 - accuracy: 0.8000 - val_loss: 0.4830 - val_accuracy: 0.7738\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 8s 11ms/step - loss: 0.4035 - accuracy: 0.8198 - val_loss: 0.4789 - val_accuracy: 0.7688\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 8s 11ms/step - loss: 0.3693 - accuracy: 0.8375 - val_loss: 0.4888 - val_accuracy: 0.7742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Klasifikasi menggunakan model CNN dengan file glove.6B.200d.txt\n",
        "!python3 code/cnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbd72EX362aa",
        "outputId": "570afe99-a958-42d6-afc9-cd39ee398e9e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-22 08:37:07.382869: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Epoch 1/8\n",
            "704/704 [==============================] - 27s 27ms/step - loss: 0.5313 - accuracy: 0.7250 - val_loss: 0.4677 - val_accuracy: 0.7744\n",
            "Epoch 2/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.4496 - accuracy: 0.7895 - val_loss: 0.4859 - val_accuracy: 0.7565\n",
            "Epoch 3/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.4005 - accuracy: 0.8179 - val_loss: 0.4599 - val_accuracy: 0.7823\n",
            "Epoch 4/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.3502 - accuracy: 0.8441 - val_loss: 0.4721 - val_accuracy: 0.7752\n",
            "Epoch 5/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.3077 - accuracy: 0.8647 - val_loss: 0.5109 - val_accuracy: 0.7759\n",
            "Epoch 6/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2733 - accuracy: 0.8819 - val_loss: 0.5617 - val_accuracy: 0.7698\n",
            "Epoch 7/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2442 - accuracy: 0.8952 - val_loss: 0.6093 - val_accuracy: 0.7656\n",
            "Epoch 8/8\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.2224 - accuracy: 0.9065 - val_loss: 0.6446 - val_accuracy: 0.7642\n"
          ]
        }
      ]
    }
  ]
}